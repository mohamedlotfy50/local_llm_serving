{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-03T13:53:34.494565Z",
     "iopub.status.busy": "2025-04-03T13:53:34.494285Z",
     "iopub.status.idle": "2025-04-03T13:53:42.438700Z",
     "shell.execute_reply": "2025-04-03T13:53:42.437791Z",
     "shell.execute_reply.started": "2025-04-03T13:53:34.494535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:57:55.270508Z",
     "iopub.status.busy": "2025-04-03T13:57:55.270202Z",
     "iopub.status.idle": "2025-04-03T13:57:55.274090Z",
     "shell.execute_reply": "2025-04-03T13:57:55.273231Z",
     "shell.execute_reply.started": "2025-04-03T13:57:55.270481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "output_dir =\"assets\\\\Qwen2_0.5B_Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:57:56.583496Z",
     "iopub.status.busy": "2025-04-03T13:57:56.583175Z",
     "iopub.status.idle": "2025-04-03T13:58:00.214614Z",
     "shell.execute_reply": "2025-04-03T13:58:00.213756Z",
     "shell.execute_reply.started": "2025-04-03T13:57:56.583470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# snapshot_download(repo_id=model_name,local_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:58:04.661369Z",
     "iopub.status.busy": "2025-04-03T13:58:04.661094Z",
     "iopub.status.idle": "2025-04-03T13:58:19.712849Z",
     "shell.execute_reply": "2025-04-03T13:58:19.712119Z",
     "shell.execute_reply.started": "2025-04-03T13:58:04.661347Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    device_map=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 1.840416GB\n",
      "torch.cuda.memory_reserved: 2.371094GB\n",
      "torch.cuda.max_memory_reserved: 2.371094GB\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:51:25.140648Z",
     "iopub.status.busy": "2025-04-03T13:51:25.140069Z",
     "iopub.status.idle": "2025-04-03T13:51:29.083579Z",
     "shell.execute_reply": "2025-04-03T13:51:29.082602Z",
     "shell.execute_reply.started": "2025-04-03T13:51:25.140621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.tokenization_qwen2_fast.Qwen2TokenizerFast"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:51:30.255317Z",
     "iopub.status.busy": "2025-04-03T13:51:30.254999Z",
     "iopub.status.idle": "2025-04-03T13:51:30.259314Z",
     "shell.execute_reply": "2025-04-03T13:51:30.258034Z",
     "shell.execute_reply.started": "2025-04-03T13:51:30.255289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:51:31.402138Z",
     "iopub.status.busy": "2025-04-03T13:51:31.401857Z",
     "iopub.status.idle": "2025-04-03T13:51:34.176418Z",
     "shell.execute_reply": "2025-04-03T13:51:34.175530Z",
     "shell.execute_reply.started": "2025-04-03T13:51:31.402119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Give me a short introduction to large language model.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:51:36.340098Z",
     "iopub.status.busy": "2025-04-03T13:51:36.339730Z",
     "iopub.status.idle": "2025-04-03T13:51:36.349541Z",
     "shell.execute_reply": "2025-04-03T13:51:36.348600Z",
     "shell.execute_reply.started": "2025-04-03T13:51:36.340072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    [messages,messages],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n',\n",
       " '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:51:38.166146Z",
     "iopub.status.busy": "2025-04-03T13:51:38.165852Z",
     "iopub.status.idle": "2025-04-03T13:51:38.173117Z",
     "shell.execute_reply": "2025-04-03T13:51:38.172217Z",
     "shell.execute_reply.started": "2025-04-03T13:51:38.166126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([text,text], return_tensors=\"pt\").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T13:51:40.336283Z",
     "iopub.status.busy": "2025-04-03T13:51:40.335890Z",
     "iopub.status.idle": "2025-04-03T13:51:43.464598Z",
     "shell.execute_reply": "2025-04-03T13:51:43.463679Z",
     "shell.execute_reply.started": "2025-04-03T13:51:40.336252Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 1.848357GB\n",
      "torch.cuda.memory_reserved: 2.388672GB\n",
      "torch.cuda.max_memory_reserved: 2.388672GB\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llm_server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
